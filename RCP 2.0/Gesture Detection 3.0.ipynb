{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Landmarks using Mediapipe and YOLOv5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediapipe currently does not support detection of multiple persons in a single frame. YOLOv5 is used to detect the various subjects and crop out the relevant bounding box involving each of them. Mediapipe will then detect the hand, post and facial landmarks using the holistic model, saving the information in a csv file, with each frame having a seperate csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DOs\n",
    "### Features\n",
    "1) Calculate FPS (DONE)\n",
    "\n",
    "2) Assign \"index\" to person ID\n",
    "\n",
    "3) Store Landmark values and impute missing values\n",
    "\n",
    "4) Feature engineering for relevant angles and distances\n",
    "\n",
    "5) Train model based on engineered features\n",
    "\n",
    "6) Create a window based detection algorithm on model output\n",
    "\n",
    "7) Store prediction and relevant intermediate data in CSV, one file for each frame\n",
    "\n",
    "### Issues\n",
    "1) Slow interference by YOLOv5 model\n",
    "\n",
    "2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chanchialer/opt/anaconda3/envs/RCP/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/chanchialer/opt/anaconda3/envs/RCP/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "#Import relevant libraries\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "from sort import *\n",
    "# import PIL\n",
    "# from PIL import Image\n",
    "# from matplotlib import pyplot\n",
    "# import matplotlib.image as mpimg\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/chanchialer/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirements \"scipy>=1.4.1\" \"seaborn>=0.11.0\" not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (from scipy>=1.4.1) (1.23.4)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (from seaborn>=0.11.0) (1.5.0)\n",
      "Requirement already satisfied: matplotlib>=3.1 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (from seaborn>=0.11.0) (3.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.25->seaborn>=0.11.0) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.25->seaborn>=0.11.0) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn>=0.11.0) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn>=0.11.0) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.1->seaborn>=0.11.0) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/chanchialer/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=0.25->seaborn>=0.11.0) (1.15.0)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m 2 packages updated per /Users/chanchialer/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "YOLOv5 üöÄ 2022-10-15 Python-3.8.8 torch-1.12.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # or yolov5n - yolov5x6, custom\n",
    "#since we are only intrested in detecting person\n",
    "yolo_model.classes=[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "# mp_hol = mp.solutions.holistic\n",
    "# mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "#generates a new list containing xmin, ymin, xmax,   ymax,  index, confidence\n",
    "#class is not included as an output since we are dealing with only perons\n",
    "def bbs_info_generator(bbs_with_ids, bbs_with_confidence):\n",
    "    if len(bbs_with_ids) != len(bbs_with_confidence):\n",
    "        return\n",
    "    output = []\n",
    "    for i in range(len(bbs_with_ids)):\n",
    "        bb_info = bbs_with_ids[i][0:].copy()\n",
    "        bb_info.extend([bbs_with_confidence[i][4]])\n",
    "        output.append(bb_info)\n",
    "    return output\n",
    "\n",
    "#Stores information about landmarks for a person in a nested list\n",
    "#each item in the list represents a landmark and contains the [x,y,z,visibility] of the landmark\n",
    "def person_landmark_info_generator(results,index):\n",
    "    person_landmark_info = []\n",
    "    if results.pose_landmarks != None:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            landmark_info = []\n",
    "            landmark_info.append(landmark.x)\n",
    "            landmark_info.append(landmark.y)\n",
    "            landmark_info.append(landmark.z)\n",
    "            landmark_info.append(landmark.visibility)\n",
    "            landmark_info.append(index)\n",
    "            person_landmark_info.append(landmark_info)\n",
    "    else:\n",
    "        for landmark in range(33):\n",
    "            person_landmark_info.append([np.nan,np.nan,np.nan,np.nan,index])\n",
    "\n",
    "    if results.face_landmarks != None:\n",
    "        for landmark in results.face_landmarks.landmark:\n",
    "            landmark_info = []\n",
    "            landmark_info.append(landmark.x)\n",
    "            landmark_info.append(landmark.y)\n",
    "            landmark_info.append(landmark.z)\n",
    "            landmark_info.append(landmark.visibility)\n",
    "            landmark_info.append(index)\n",
    "            person_landmark_info.append(landmark_info)\n",
    "    else:\n",
    "        for landmark in range(468):\n",
    "            person_landmark_info.append([np.nan,np.nan,np.nan,np.nan,index])\n",
    "\n",
    "    if results.left_hand_landmarks != None:\n",
    "        for landmark in results.left_hand_landmarks.landmark:\n",
    "            landmark_info = []\n",
    "            landmark_info.append(landmark.x)\n",
    "            landmark_info.append(landmark.y)\n",
    "            landmark_info.append(landmark.z)\n",
    "            landmark_info.append(landmark.visibility)\n",
    "            landmark_info.append(index)\n",
    "            person_landmark_info.append(landmark_info)\n",
    "    else:\n",
    "        for landmark in range(21):\n",
    "            person_landmark_info.append([np.nan,np.nan,np.nan,np.nan,index])       \n",
    "\n",
    "    if results.right_hand_landmarks != None:\n",
    "        for landmark in results.right_hand_landmarks.landmark:\n",
    "            landmark_info = []\n",
    "            landmark_info.append(landmark.x)\n",
    "            landmark_info.append(landmark.y)\n",
    "            landmark_info.append(landmark.z)\n",
    "            landmark_info.append(landmark.visibility)\n",
    "            landmark_info.append(index)\n",
    "            person_landmark_info.append(landmark_info)\n",
    "    else:\n",
    "        for landmark in range(21):\n",
    "            person_landmark_info.append([np.nan,np.nan,np.nan,np.nan,index])      \n",
    "    return person_landmark_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(person_landmark_info, columns=[\n",
    "#     str(\"person\" + str(index) + \".x\"), str(\"person\" + str(index) + \".y\"), \n",
    "#     str(\"person\" + str(index) + \".z\"), str(\"person\" + str(index) + \".visibility\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 720)\n"
     ]
    }
   ],
   "source": [
    "video_path =\"/Users/chanchialer/Documents/GitHub/RCP/RCP 2.0/test.mp4\"\n",
    "# video_path =\"/Users/chanchialer/Downloads/Baby.mp4\"\n",
    "\n",
    "#get the dimension of the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    h, w, _ = frame.shape\n",
    "    size = (w, h)\n",
    "    print(size)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# used to record the time when we processed last frame\n",
    "prev_frame_time = 0\n",
    " \n",
    "# used to record the time at which we processed current frame\n",
    "new_frame_time = 0\n",
    "\n",
    "#create instance of SORT\n",
    "mot_tracker = Sort() \n",
    "\n",
    "mp_hol = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "#For saving the video file as output.avi\n",
    "out = cv2.VideoWriter(\"output.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), 20, size)\n",
    "\n",
    "all_frame_landmark_info = []\n",
    "\n",
    "frame_count = 0\n",
    "with mp_hol.Holistic(min_detection_confidence=0.3, min_tracking_confidence=0.2) as holistic:\n",
    "    while cap.isOpened():    \n",
    "        ret, frame = cap.read()  \n",
    "        if ret == False:\n",
    "            break\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        frame_count += 1\n",
    "\n",
    "        # Recolor Feed from RGB to BGR\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        #making image writeable to false improves prediction\n",
    "        image.flags.writeable = False    \n",
    "\n",
    "        # get detections\n",
    "        result = yolo_model(image)  \n",
    "        detections = result.pred[0].numpy()\n",
    "        \n",
    "        # update SORT\n",
    "        track_bbs_ids = mot_tracker.update(detections)\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        #This array will contain crops of images incase we need it \n",
    "        img_list =[]\n",
    "\n",
    "        #we need some extra margin bounding box for human crops to be properly detected\n",
    "        MARGIN=10\n",
    "        \n",
    "        bbs_info = []\n",
    "        bbs_info = bbs_info_generator(track_bbs_ids.tolist(), result.xyxy[0].tolist())\n",
    "        \n",
    "        all_person_landmark_info = []\n",
    "        \n",
    "        if bbs_info != None:\n",
    "            for (xmin, ymin, xmax,   ymax,  index,  confidence) in bbs_info:\n",
    "                results = holistic.process(image[int(ymin)+MARGIN:int(ymax)+MARGIN,int(xmin)+MARGIN:int(xmax)+MARGIN:])\n",
    "                \n",
    "                #Draw bounding box on image\n",
    "                cv2.rectangle(image, (int(xmin),int(ymin)), (int(xmax), int(ymax)), (255,0,0), 2)\n",
    "                cv2.putText(image, str(int(index)), (int(xmax), int(ymax)), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0), 2)\n",
    "                \n",
    "                #Draw pose landmarks on image\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image[int(ymin)+MARGIN:int(ymax)+MARGIN,int(xmin)+MARGIN:int(xmax)+MARGIN:], \n",
    "                    results.pose_landmarks, \n",
    "                    mp_hol.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                    mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                ) \n",
    "                #collect information on landmarks of the person detected\n",
    "                person_landmark_info = person_landmark_info_generator(results, index)\n",
    "#                 df_person_landmark_info = pd.DataFrame(person_landmark_info, columns=[\n",
    "#                                         str(\"person\" + str(index) + \".x\"), str(\"person\" + str(index) + \".y\"), \n",
    "#                                         str(\"person\" + str(index) + \".z\"), str(\"person\" + str(index) + \".visibility\")])\n",
    "                \n",
    "                #store this landmark infomration in a list\n",
    "#                 df_all_person_landmark_info = pd.concat([df_all_person_landmark_info, df_person_landmark_info], axis=1)\n",
    "                all_person_landmark_info.append(person_landmark_info)\n",
    "                \n",
    "#                 #Draw face landmarks on image  \n",
    "#                 mp_drawing.draw_landmarks(\n",
    "#                     image[int(ymin)+MARGIN:int(ymax)+MARGIN,int(xmin)+MARGIN:int(xmax)+MARGIN:], \n",
    "#                     results.face_landmarks, \n",
    "#                     mp_hol.FACEMESH_CONTOURS,\n",
    "#                     mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "#                     mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "#                 ) \n",
    "#                 #Draw left hand landmarks on image  \n",
    "#                 mp_drawing.draw_landmarks(\n",
    "#                     image[int(ymin)+MARGIN:int(ymax)+MARGIN,int(xmin)+MARGIN:int(xmax)+MARGIN:], \n",
    "#                     results.left_hand_landmarks, \n",
    "#                     mp_hol.HAND_CONNECTIONS,\n",
    "#                     mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "#                     mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "#                 ) \n",
    "#                 #Draw right hand landmarks on image  \n",
    "#                 mp_drawing.draw_landmarks(\n",
    "#                     image[int(ymin)+MARGIN:int(ymax)+MARGIN,int(xmin)+MARGIN:int(xmax)+MARGIN:], \n",
    "#                     results.right_hand_landmarks, \n",
    "#                     mp_hol.HAND_CONNECTIONS,\n",
    "#                     mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "#                     mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "#                 ) \n",
    "\n",
    "        #         img_list.append(image[int(ymin):int(ymax),int(xmin):int(xmax):])\n",
    "        \n",
    "        \n",
    "        all_frame_landmark_info.append(all_person_landmark_info)\n",
    "        \n",
    "        new_frame_time = time.time()\n",
    "        fps = 1/(new_frame_time-prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        cv2.putText(image, str(int(fps)), (10,70), cv2.FONT_HERSHEY_PLAIN, 3, (0, 0, 0), 3)\n",
    "        cv2.putText(image, str(frame_count), (w-140, 70), cv2.FONT_HERSHEY_PLAIN, 3, (0, 0, 0), 3)\n",
    "        # Write the frame into the file 'output.avi'\n",
    "        out.write(image)\n",
    "        cv2.imshow(\"image\", image)\n",
    "\n",
    "            # writing in the video file \n",
    "        #     out.write(image)\n",
    "\n",
    "\n",
    "        # if the 'q' key is pressed, stop the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "# When everything done, release the video capture and video write objects\n",
    "cap.release()\n",
    "out.release()         \n",
    "del mot_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_person1_info = []\n",
    "all_person2_info = []\n",
    "\n",
    "for frame in all_frame_landmark_info:\n",
    "    person1_info = []\n",
    "    person2_info = []\n",
    "    for person in frame:\n",
    "        for landmark in person:\n",
    "            if landmark[4] == 1:\n",
    "                person1_info.extend(landmark[0:4])\n",
    "            if landmark[4] == 2:\n",
    "                person2_info.extend(landmark[0:4])\n",
    "    all_person1_info.append(person1_info)\n",
    "    all_person2_info.append(person2_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = []\n",
    "for val in range(1, len(all_frame_landmark_info[0][0])+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_person1_info = pd.DataFrame(data=all_person1_info, columns = landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_person2_info = pd.DataFrame(data=all_person2_info, columns = landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_person1_info = df_all_person1_info.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_person2_info = df_all_person2_info.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_person1_info.insert(0,\"point\", \" \")\n",
    "df_all_person1_info.insert(0,\"reach\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_person2_info.insert(0,\"point\", \" \")\n",
    "df_all_person2_info.insert(0,\"reach\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_person1_info.to_csv('person1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_person2_info.to_csv('person2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
